{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-04 基于深度学习的文本分类1-fastText\n",
    "与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。从本章开始我们将学习如何使用深度学习来完成文本表示。\n",
    "\n",
    "## 学习目标\n",
    "- 学习FastText的使用和基础原理\n",
    "- 学会使用验证集进行调参\n",
    "\n",
    "## 文本表示方法 Part2\n",
    "### 现有文本表示方法的缺陷\n",
    "在上一章节，我们介绍几种文本表示方法：\n",
    "\n",
    "- One-hot\n",
    "- Bag of Words\n",
    "- N-gram\n",
    "- TF-IDF\n",
    "\n",
    "也通过sklean进行了相应的实践，相信你也有了初步的认知。但上述方法都或多或少存在一定的问题：转换得到的向量维度很高，需要较长的训练实践；没有考虑单词与单词之间的关系，只是进行了统计。\n",
    "\n",
    "与这些表示方法不同，深度学习也可以用于文本表示，还可以将其映射到一个低纬空间。其中比较典型的例子有：**FastText**、**Word2Vec**和**Bert**。在本章我们将介绍**FastText**，将在后面的内容介绍**Word2Vec**和**Bert**。\n",
    "\n",
    "### FastText\n",
    "FastText是一种典型的深度学习词向量的表示方法，它非常简单通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。\n",
    "\n",
    "所以FastText是一个三层的神经网络，输入层、隐含层和输出层。\n",
    "\n",
    "\n",
    "![fast_text](https://img-blog.csdnimg.cn/20200714204856589.png)\n",
    "\n",
    "下图是使用keras实现的FastText网络结构：\n",
    "\n",
    "![keras_fasttext](https://img-blog.csdnimg.cn/20200714204249463.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# coding: utf-8\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "\n",
    "VOCAB_SIZE = 2000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_WORDS = 500\n",
    "CLASS_NUM = 5\n",
    "\n",
    "def build_fastText():\n",
    "    model = Sequential()\n",
    "    # 通过 embedding层，我们将词汇映射成 EMBEDDING_DIM维向量\n",
    "    model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_legth=MAX_WORDS))\n",
    "    # 通过 GlobalAveragePooling1D，我们平均了文档中所有词的embedding\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    # 通过输出层 Softmax分类（真实的fastText这里是分层Softmax），得到类别概率分布\n",
    "    model.add(Dense(CLASS_NUM, activation='softmax'))\n",
    "    # 定义损失函数、优化器、分类度量指标\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = build_fastText()\n",
    "    print(model.sumary())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText在文本分类任务上，是优于TF-IDF的：\n",
    "\n",
    "- FastText用单词的Embedding叠加获得的文档向量，将相似的句子分为一类\n",
    "- FastText学习到的Embedding空间维度比较低，可以快速进行训练\n",
    "\n",
    "如果想深度学习，可以参考论文：\n",
    "\n",
    "Bag of Tricks for Efficient Text Classification, <https://arxiv.org/abs/1607.01759>\n",
    "\n",
    "## 基于FastText的文本分类\n",
    "FastText可以快速的在CPU上进行训练，最好的实践方法就是官方开源的版本： <https://github.com/facebookresearch/fastText/tree/master/python>\n",
    "\n",
    "- pip 安装\n",
    "\n",
    "`pip install fasttext`\n",
    "\n",
    "- 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8392154026731075\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 转换为FastText需要的格式\n",
    "train_df = pd.read_csv('./input/train_set.csv', sep='\\t', nrows=15000)\n",
    "train_df['label_ft'] = '__label__' + train_df['label'].astype(str)\n",
    "train_df[['text','label_ft']].iloc[:-3000].to_csv('train.csv', index=None, header=None, sep='\\t')\n",
    "\n",
    "import fasttext\n",
    "model = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, \n",
    "                                  verbose=2, minCount=1, epoch=25, loss=\"hs\")\n",
    "\n",
    "val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-3000:]['text']]\n",
    "print(f1_score(train_df['label'].values[-3000:].astype(str), val_pred, average='macro'))\n",
    "# 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时数据量比较小得分为0.82，当不断增加训练集数量时，FastText的精度也会不断增加5w条训练样本时，验证集得分可以到0.89-0.90左右。\n",
    "\n",
    "## 如何使用验证集调参\n",
    "在使用TF-IDF和FastText中，有一些模型的参数需要选择，这些参数会在一定程度上影响模型的精度，那么如何选择这些参数呢？\n",
    "\n",
    "- 通过阅读文档，要弄清楚这些参数的大致含义，那些参数会增加模型的复杂度\n",
    "- 通过在验证集上进行验证模型精度，找到模型在是否过拟合还是欠拟合\n",
    "\n",
    "\n",
    "![train_val](https://img-blog.csdnimg.cn/20200714204403844.png)\n",
    "\n",
    "\n",
    "这里我们使用10折交叉验证，每折使用9/10的数据进行训练，剩余1/10作为验证集检验模型的效果。这里需要注意每折的划分必须保证标签的分布与整个数据集的分布一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "label2id = {}\n",
    "for i in range(total):\n",
    "    label = str(all_labels[i])\n",
    "    if label not in label2id:\n",
    "        label2id[label] = [i]\n",
    "    else:\n",
    "        label2id[label].append(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过10折划分，我们一共得到了10份分布一致的数据，索引分别为0到9，每次通过将一份数据作为验证集，剩余数据作为训练集，获得了所有数据的10种分割。不失一般性，我们选择最后一份完成剩余的实验，即索引为9的一份做为验证集，索引为1-8的作为训练集，然后基于验证集的结果调整超参数，使得模型性能更优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本章小结\n",
    "本章介绍了FastText的原理和基础使用，并进行相应的实践。然后介绍了通过10折交叉验证划分数据集。\n",
    "\n",
    "## 本章作业\n",
    "- 阅读FastText的文档，尝试修改参数，得到更好的分数\n",
    "- 基于验证集的结果调整超参数，使得模型性能更优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8776076770442628\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 转换为FastText需要的格式\n",
    "train_df = pd.read_csv('./input/train_set.csv', sep='\\t', nrows=15000)\n",
    "train_df['label_ft'] = '__label__' + train_df['label'].astype(str)\n",
    "train_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')\n",
    "\n",
    "import fasttext\n",
    "model = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, loss=\"softmax\",\n",
    "                                  verbose=2, minCount=1, epoch=50)\n",
    "\n",
    "val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]\n",
    "print(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro'))\n",
    "# 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fasttext相关文档](https://pypi.org/project/fasttext/)\n",
    "\n",
    "```python\n",
    "def train_supervised(input, lr=0.1, dim=100, \n",
    "                   ws=5, epoch=5, minCount=1, \n",
    "                   minCountLabel=0, minn=0, \n",
    "                   maxn=0, neg=5, wordNgrams=1, \n",
    "                   loss=\"softmax\", bucket=2000000, \n",
    "                   thread=12, lrUpdateRate=100,\n",
    "                   t=1e-4, label=\"__label__\", \n",
    "                   verbose=2, pretrainedVectors=\"\"):\n",
    "  \"\"\"\n",
    "  训练一个监督模型, 返回一个模型对象\n",
    "\n",
    "  @param input: 训练数据文件路径\n",
    "  @param lr:              学习率\n",
    "  @param dim:             向量维度\n",
    "  @param ws:              cbow模型时使用\n",
    "  @param epoch:           次数\n",
    "  @param minCount:        词频阈值, 小于该值在初始化时会过滤掉\n",
    "  @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉\n",
    "  @param minn:            构造subword时最小char个数\n",
    "  @param maxn:            构造subword时最大char个数\n",
    "  @param neg:             负采样\n",
    "  @param wordNgrams:      n-gram个数\n",
    "  @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax\n",
    "  @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量\n",
    "  @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出\n",
    "  @param lrUpdateRate:    学习率更新\n",
    "  @param t:               负采样阈值\n",
    "  @param label:           类别前缀\n",
    "  @param verbose:         ??\n",
    "  @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机\n",
    "  @return model object\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "input             # training file path (required)\n",
    "lr                # learning rate [0.1]\n",
    "dim               # size of word vectors [100]\n",
    "ws                # size of the context window [5]\n",
    "epoch             # number of epochs [5]\n",
    "minCount          # minimal number of word occurences [1]\n",
    "minCountLabel     # minimal number of label occurences [1]\n",
    "minn              # min length of char ngram [0]\n",
    "maxn              # max length of char ngram [0]\n",
    "neg               # number of negatives sampled [5]\n",
    "wordNgrams        # max length of word ngram [1]\n",
    "loss              # loss function {ns, hs, softmax, ova} [softmax]\n",
    "bucket            # number of buckets [2000000]\n",
    "thread            # number of threads [number of cpus]\n",
    "lrUpdateRate      # change the rate of updates for the learning rate [100]\n",
    "t                 # sampling threshold [0.0001]\n",
    "label             # label prefix ['__label__']\n",
    "verbose           # verbose [2]\n",
    "pretrainedVectors # pretrained word vectors (.vec file) for supervised learning []\n",
    "\n",
    "def model.predict(self, text, k=1, threshold=0.0):\n",
    "  \"\"\"\n",
    "  模型预测，给定文本预测分类\n",
    "\n",
    "  @param text:       字符串, 需要utf-8\n",
    "  @param k:          返回标签的个数\n",
    "  @param threshold   概率阈值, 大于该值才返回\n",
    "  @return 标签列表, 概率列表\n",
    "  \"\"\"\n",
    "\n",
    "```\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
